{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference PyTorch GPT2 Model with ONNX Runtime on CPU\n",
    "\n",
    "In this tutorial, you'll be introduced to how to load a GPT2 model from PyTorch, convert it to ONNX, and inference it using ONNX Runtime.\n",
    "\n",
    "**Note: this work is still in progresss. Need install ort_nightly package before onnxruntime 1.3.0 is ready. The performance number here does not reflect the final result for onnxruntime 1.3.0. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites ##\n",
    "\n",
    "If you have Jupyter Notebook, you may directly run this notebook. We will use pip to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/) and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. First, we install [AnaConda](https://www.anaconda.com/distribution/). Then open an AnaConda prompt window and run the following commands:\n",
    "\n",
    "```console\n",
    "conda create -n cpu_env python=3.6\n",
    "conda activate cpu_env\n",
    "\n",
    "conda install pytorch torchvision cpuonly -c pytorch\n",
    "pip install onnxruntime\n",
    "pip install transformers==2.5.1\n",
    "pip install onnx psutil pytz pandas py-cpuinfo py3nvml netron\n",
    "\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "The last command will launch Jupyter Notebook and we can open this notebook in browser to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable pass state in input.\n",
    "enable_past_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = \"./gpt2\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "output_dir = './gpt2_onnx'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark ##\n",
    "\n",
    "You will need git clone the onnxruntime repository like\n",
    "```console\n",
    "git clone https://github.com/microsoft/onnxruntime.git\n",
    "```\n",
    "Then update the bert_tools_dir according to the path in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   benchmark_gpt2.py: no environment variable of OMP_NUM_THREADS\n",
      "   benchmark_gpt2.py: no environment variable of OMP_WAIT_POLICY\n",
      "tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at ./gpt2\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at ./gpt2\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at ./gpt2\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "configuration_utils.py: Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "   modeling_utils.py: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at ./gpt2\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\transformers\\modeling_gpt2.py:147: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  w = w / math.sqrt(v.size(-1))\n",
      "D:\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\transformers\\modeling_gpt2.py:149: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  b = self.bias[:, :, ns-nd:ns, :ns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   benchmark_gpt2.py: PyTorch Inference time = 36.92 ms\n",
      "   benchmark_gpt2.py: OMP_NUM_THREADS=1\n",
      "   benchmark_gpt2.py: OMP_WAIT_POLICY=ACTIVE\n",
      "   benchmark_gpt2.py: Prune graph to keep the first output and drop past state outputs:['last_state']\n",
      "        OnnxModel.py: Graph pruned: 0 inputs, 12 outputs and 48 nodes are removed\n",
      "        OnnxModel.py: Output model to ./gpt2_onnx\\gpt2_past0_out1.onnx\n",
      "    BertOnnxModel.py: Fused LayerNormalization count: 25\n",
      "    BertOnnxModel.py: Fused FastGelu count: 12\n",
      "    BertOnnxModel.py: Fused Reshape count:48\n",
      "        OnnxModel.py: Graph pruned: 0 inputs, 0 outputs and 1106 nodes are removed\n",
      "    Gpt2OnnxModel.py: Fused Attention count:12\n",
      "    BertOnnxModel.py: Failed to find embedding layer\n",
      "    BertOnnxModel.py: Fused EmbedLayerNormalization count: 0\n",
      "        OnnxModel.py: Graph pruned: 0 inputs, 0 outputs and 480 nodes are removed\n",
      "    Gpt2OnnxModel.py: Remove Reshape count:48\n",
      "    BertOnnxModel.py: Fused FastGelu with Bias count:12\n",
      "    BertOnnxModel.py: opset verion: 11\n",
      "        OnnxModel.py: Output model to ./gpt2_onnx\\gpt2_past0_optimized.onnx\n",
      "   benchmark_gpt2.py: session option: intra_op_num_threads=12\n",
      "   benchmark_gpt2.py: Start inferencing onnx model: ./gpt2_onnx\\gpt2_past0_optimized.onnx\n",
      "   benchmark_gpt2.py: OnnxRuntime Inference time = 60.92 ms\n"
     ]
    }
   ],
   "source": [
    "# Assume you have git clone the repository of onnxruntime from github.\n",
    "bert_tools_dir = r'D:\\Git\\onnxruntime\\onnxruntime\\python\\tools\\bert'\n",
    "benchmark_script = os.path.join(bert_tools_dir, 'benchmark_gpt2.py')\n",
    "\n",
    "if enable_past_input:\n",
    "    %run $benchmark_script --model_type gpt2 --cache_dir $cache_dir --output_dir $output_dir --enable_optimization --enable_past_input\n",
    "else:\n",
    "    %run $benchmark_script --model_type gpt2 --cache_dir $cache_dir --output_dir $output_dir --enable_optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only need the benchmark results. You can skip the remaining parts.\n",
    "\n",
    "In the following, we will introduce the benchmark script.\n",
    "\n",
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at ./gpt2\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "tokenization_utils.py: loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at ./gpt2\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "configuration_utils.py: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at ./gpt2\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "configuration_utils.py: Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "   modeling_utils.py: loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at ./gpt2\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "model_class, tokenizer_class,  model_name_or_path = (GPT2Model,  GPT2Tokenizer,  'gpt2')\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model = model_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "model.eval().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "def pytorch_inference(model, input_ids, past=None, total_runs = 100):\n",
    "    latency = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(total_runs):\n",
    "            start = time.time()\n",
    "            outputs = model(input_ids=input_ids, past=past)\n",
    "            latency.append(time.time() - start)\n",
    "            \n",
    "    if total_runs > 1:\n",
    "        print(\"PyTorch Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))\n",
    "    \n",
    "    return outputs\n",
    "    \n",
    "def onnxruntime_inference(ort_session, input_ids, past=None, total_runs=100):    \n",
    "    # Use contiguous array as input might improve performance.\n",
    "    # You can check the results from performance test tool to see whether you need it.\n",
    "    ort_inputs = {\n",
    "        'input_ids':  numpy.ascontiguousarray(input_ids.cpu().numpy())\n",
    "    }\n",
    "    \n",
    "    if past is not None:\n",
    "        for i, past_i in enumerate(past):\n",
    "            ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past[i].cpu().numpy())\n",
    "            \n",
    "    latency = []\n",
    "    for _ in range(total_runs):\n",
    "        start = time.time()\n",
    "        ort_outputs = ort_session.run(None, ort_inputs)\n",
    "        latency.append(time.time() - start)\n",
    "        \n",
    "    if total_runs > 1:\n",
    "        print(\"OnnxRuntime Inference time = {} ms\".format(format(sum(latency) * 1000 / len(latency), '.2f')))\n",
    "    \n",
    "    return ort_outputs\n",
    "\n",
    "def inference(model, ort_session, input_ids, past=None, total_runs=100, verify_outputs=True):\n",
    "    outputs = pytorch_inference(model, input_ids, past, total_runs)\n",
    "    ort_outputs = onnxruntime_inference(ort_session, input_ids, past, total_runs)\n",
    "    if verify_outputs:\n",
    "        print('PyTorch and OnnxRuntime output 0 (last_state) are close:'.format(0), numpy.allclose(ort_outputs[0], outputs[0].cpu(), rtol=1e-05, atol=1e-04))\n",
    "\n",
    "        if enable_past_input:\n",
    "            for layer in range(model.config.n_layer):\n",
    "                print('PyTorch and OnnxRuntime layer {} state (present_{}) are close:'.format(layer, layer), numpy.allclose(ort_outputs[1 + layer], outputs[1][layer].cpu(), rtol=1e-05, atol=1e-04))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "inputs = tokenizer.encode_plus(\"Here is an example input for GPT2 model\", add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# run without past so that we can know the shape of past from output.\n",
    "outputs = model(input_ids=input_ids, past=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = model.config.n_layer    \n",
    "present_names = [f'present_{i}' for i in range(num_layer)]\n",
    "output_names = [\"last_state\"] + present_names\n",
    "\n",
    "input_names = ['input_ids']\n",
    "dynamic_axes= {'input_ids': {0: 'batch_size', 1: 'seq_len'},\n",
    "               #'token_type_ids' : {0: 'batch_size', 1: 'seq_len'},\n",
    "               #'attention_mask' : {0: 'batch_size', 1: 'seq_len'},\n",
    "               'last_state' : {0: 'batch_size', 1: 'seq_len'}\n",
    "              }\n",
    "for name in present_names:\n",
    "        dynamic_axes[name] = {1: 'batch_size', 3: 'seq_len'}\n",
    "        \n",
    "if enable_past_input:\n",
    "    past_names = [f'past_{i}' for i in range(num_layer)]\n",
    "    input_names = ['input_ids'] + past_names  #+ ['token_type_ids', 'attention_mask']\n",
    "    dummy_past = [torch.zeros(list(outputs[1][0].shape)) for _ in range(num_layer)]\n",
    "    for name in past_names:\n",
    "        dynamic_axes[name] = {1: 'batch_size', 3: 'seq_len'}\n",
    "    export_inputs = (inputs['input_ids'], tuple(dummy_past)) #, inputs['token_type_ids'], inputs['attention_mask'])\n",
    "else:\n",
    "    export_inputs = (inputs['input_ids'])\n",
    "\n",
    "export_model_path = os.path.join(output_dir, 'gpt2_past{}.onnx'.format(int(enable_past_input)))\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                  args=export_inputs,\n",
    "                  f=export_model_path,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names,\n",
    "                  dynamic_axes=dynamic_axes,\n",
    "                  opset_version=11,\n",
    "                  do_constant_folding = True,\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " remove_past_outputs: Prune graph to keep the first output and drop past state outputs:['last_state']\n",
      "         prune_graph: Graph pruned: 0 inputs, 12 outputs and 48 nodes are removed\n",
      "  save_model_to_file: Output model to ./gpt2_onnx\\gpt2_past0_out1.onnx\n"
     ]
    }
   ],
   "source": [
    "def remove_past_outputs(export_model_path, output_model_path):\n",
    "    from onnx import ModelProto\n",
    "    from OnnxModel import OnnxModel\n",
    "\n",
    "    model = ModelProto()\n",
    "    with open(export_model_path, \"rb\") as f:\n",
    "        model.ParseFromString(f.read())\n",
    "    bert_model = OnnxModel(model)\n",
    "\n",
    "    # remove past state outputs and only keep the first output.\n",
    "    keep_output_names = [bert_model.model.graph.output[0].name]\n",
    "    logger.info(f\"Prune graph to keep the first output and drop past state outputs:{keep_output_names}\")\n",
    "    bert_model.prune_graph(keep_output_names)\n",
    "\n",
    "    bert_model.save_model_to_file(output_model_path)\n",
    "    \n",
    "if enable_past_input:\n",
    "    onnx_model_path = export_model_path\n",
    "else:\n",
    "    onnx_model_path = os.path.join(output_dir, 'gpt2_past{}_out1.onnx'.format(int(enable_past_input)))\n",
    "    remove_past_outputs(export_model_path, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with ONNX Runtime\n",
    "\n",
    "### OpenMP Environment Variable\n",
    "\n",
    "OpenMP environment variables are very important for CPU inference of GPT2 model. It has large performance impact on GPT2 model so you might need set it carefully according to benchmark script.\n",
    "\n",
    "Setting environment variables shall be done before importing onnxruntime. Otherwise, they might not take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "# You may change the settings in this cell according to Performance Test Tool result.\n",
    "use_openmp = True\n",
    "\n",
    "# ATTENTION: these environment variables must be set before importing onnxruntime.\n",
    "if use_openmp:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = str(psutil.cpu_count(logical=True))\n",
    "else:\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inference time = 39.00 ms\n",
      "OnnxRuntime Inference time = 70.14 ms\n",
      "PyTorch and OnnxRuntime output 0 (last_state) are close: True\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy\n",
    "\n",
    "# Print warning if user uses onnxruntime-gpu instead of onnxruntime package.\n",
    "if 'CUDAExecutionProvider' in onnxruntime.get_available_providers():\n",
    "    print(\"warning: onnxruntime-gpu is not built with OpenMP. You might try onnxruntime package to test CPU inference.\")\n",
    "\n",
    "sess_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
    "# Note that this will increase session creation time, so it is for debugging only.\n",
    "#sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_cpu.onnx\")\n",
    "   \n",
    "if use_openmp:\n",
    "    sess_options.intra_op_num_threads=1\n",
    "else:\n",
    "    sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
    "\n",
    "# Specify providers when you use onnxruntime-gpu for CPU inference.\n",
    "session = onnxruntime.InferenceSession(onnx_model_path, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Compare PyTorch and OnnxRuntime inference performance and results\n",
    "%time inference(model, session, input_ids, past=dummy_past if enable_past_input else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del session\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = os.path.join(output_dir, 'gpt2_past{}_optimized.onnx'.format(int(enable_past_input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_opt_script = os.path.join(bert_tools_dir, 'bert_model_optimization.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     fuse_layer_norm: Fused LayerNormalization count: 25\n",
      " fuse_gelu_with_tanh: Fused FastGelu count: 12\n",
      "        fuse_reshape: Fused Reshape count:48\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 1106 nodes are removed\n",
      "      fuse_attention: Fused Attention count:12\n",
      "fuse_embed_layer_without_mask: Failed to find embedding layer\n",
      "    fuse_embed_layer: Fused EmbedLayerNormalization count: 0\n",
      "         prune_graph: Graph pruned: 0 inputs, 0 outputs and 480 nodes are removed\n",
      "         postprocess: Remove Reshape count:48\n",
      "      fuse_bias_gelu: Fused FastGelu with Bias count:12\n",
      "            optimize: opset verion: 11\n",
      "  save_model_to_file: Output model to ./gpt2_onnx\\gpt2_past0_optimized.onnx\n",
      "get_fused_operator_statistics: Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 12, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "  is_fully_optimized: EmbedLayer=0, Attention=12, Gelu=12, LayerNormalization=25, Successful=False\n",
      "                main: The output model is not fully optimized. It might not be usable.\n"
     ]
    }
   ],
   "source": [
    "# Local directory corresponding to https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/bert/\n",
    "%run $bert_opt_script --model_type gpt2 --input $onnx_model_path --output $optimized_model --opt_level 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Inference time = 37.91 ms\n",
      "OnnxRuntime Inference time = 66.12 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession(optimized_model, sess_options, providers=['CPUExecutionProvider'])\n",
    "\n",
    "%time inference(model, session, input_ids, past=dummy_past if enable_past_input else None, verify_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Info\n",
    "\n",
    "Note that running Jupyter Notebook has slight impact on performance result since Jupyter Notebook is using system resources like CPU and memory etc. It is recommended to close Jupyter Notebook and other applications, then run the benchmark script in a console to get more accurate performance numbers.\n",
    "\n",
    "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/master/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
    "\n",
    "Here is the machine configuration that generated the above results. The machine has GPU but not used in CPU inference.\n",
    "You might get slower or faster result based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gpu\": {\n",
      "    \"driver_version\": \"441.22\",\n",
      "    \"devices\": [\n",
      "      {\n",
      "        \"memory_total\": 8589934592,\n",
      "        \"memory_available\": 8480882688,\n",
      "        \"name\": \"GeForce GTX 1070\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"cpu\": {\n",
      "    \"brand\": \"Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz\",\n",
      "    \"cores\": 6,\n",
      "    \"logical_cores\": 12,\n",
      "    \"hz\": \"3.1920 GHz\",\n",
      "    \"l2_cache\": \"1536 KB\",\n",
      "    \"l3_cache\": \"12288 KB\",\n",
      "    \"processor\": \"Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\"\n",
      "  },\n",
      "  \"memory\": {\n",
      "    \"total\": 16971259904,\n",
      "    \"available\": 2603229184\n",
      "  },\n",
      "  \"python\": \"3.6.10.final.0 (64 bit)\",\n",
      "  \"os\": \"Windows-10-10.0.18362-SP0\",\n",
      "  \"onnxruntime\": {\n",
      "    \"version\": \"1.2.0\",\n",
      "    \"support_gpu\": false\n",
      "  },\n",
      "  \"pytorch\": {\n",
      "    \"version\": \"1.4.0+cpu\",\n",
      "    \"support_gpu\": false\n",
      "  },\n",
      "  \"tensorflow\": {\n",
      "    \"version\": \"2.1.0\",\n",
      "    \"git_version\": \"v2.1.0-rc2-17-ge5bf8de410\",\n",
      "    \"support_gpu\": true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "machine_info_script = os.path.join(bert_tools_dir, 'MachineInfo.py')\n",
    "%run $machine_info_script --silent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_env",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
