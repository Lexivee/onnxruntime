{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference PyTorch Bert Model with ONNX Runtime on GPU"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, you'll learn how to load a Bert model from PyTorch, convert it to ONNX, and inference it for high performance using ONNX Runtime and NVIDIA GPU. In the following sections, we are going to use the Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. Bert SQuAD model is used in question answering scenarios, where the answer to every question is a segment of text from the corresponding reading passage, or the question might be unanswerable.\n",
        "\n",
        "This notebook is for GPU inference. For CPU inference, please look at another notebook [Inference PyTorch Bert Model with ONNX Runtime on CPU](PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Prerequisites ##\n",
        "It requires your machine to have a GPU, and a python environment with [PyTorch](https://pytorch.org/) installed before running this notebook.\n",
        "\n",
        "#### GPU Environment Setup using AnaConda\n",
        "\n",
        "First, we install [AnaConda](https://www.anaconda.com/distribution/) in a target machine and open an AnaConda prompt window when it is done. Then run the following commands to create a conda environment. This notebook is tested with PyTorch 1.5.0 and OnnxRuntime 1.3.0.\n",
        "\n",
        "```console\n",
        "conda create -n gpu_env python=3.6\n",
        "conda activate gpu_env\n",
        "conda install -c anaconda ipykernel\n",
        "conda install -c conda-forge ipywidgets\n",
        "python -m ipykernel install --user --name=gpu_env\n",
        "jupyter notebook\n",
        "```\n",
        "Finally, launch Jupyter Notebook and you can choose gpu_env as kernel to run this notebook.\n",
        "\n",
        "Onnxruntime-gpu need specified version of CUDA and cuDNN. You can find the Requirements [here]( http://www.onnxruntime.ai/docs/how-to/install.html). Remember to add the directories to PATH environment variable (See [CUDA and cuDNN Path](#CUDA-and-cuDNN-Path) below)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "run_install = False # Only need install once\n",
        "if run_install:\n",
        "    if sys.platform in ['linux', 'win32']: # Linux or Windows\n",
        "        !{sys.executable} -m pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "    else: # Mac\n",
        "        print(\"PyTorch 1.9 MacOS Binaries do not support CUDA, install from source instead\")\n",
        "\n",
        "    !{sys.executable} -m pip install onnxruntime-gpu==1.8.1 onnx==1.9.0 onnxconverter_common==1.8.1\n",
        "\n",
        "    # Install other packages used in this notebook.\n",
        "    !{sys.executable} -m pip install transformers==4.8.2\n",
        "    !{sys.executable} -m pip install psutil pytz pandas py-cpuinfo py3nvml coloredlogs wget netron sympy"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1648061755629
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n",
        "import onnxruntime\n",
        "import transformers\n",
        "print(\"pytorch:\", torch.__version__)\n",
        "print(\"onnxruntime:\", onnxruntime.__version__)\n",
        "print(\"onnx:\", onnx.__version__)\n",
        "print(\"transformers:\", transformers.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/gpu/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "pytorch: 1.10.0\nonnxruntime: 1.11.0\nonnx: 1.11.0\ntransformers: 4.17.0\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1648061757148
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime\r\n",
        "print(onnxruntime.__name__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "onnxruntime\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1648061757560
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Pretrained Bert model ##"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by downloading the SQuAD data file and store them in the specified location. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "cache_dir = \"./squad\"\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)\n",
        "\n",
        "predict_file_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\"\n",
        "predict_file = os.path.join(cache_dir, \"dev-v1.1.json\")\n",
        "if not os.path.exists(predict_file):\n",
        "    import wget\n",
        "    print(\"Start downloading predict file.\")\n",
        "    wget.download(predict_file_url, predict_file)\n",
        "    print(\"Predict file downloaded.\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1648061757886
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first define some constant variables."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Whether allow overwriting existing ONNX model and download the latest script from GitHub\n",
        "enable_overwrite = True\n",
        "\n",
        "# Total samples to inference, so that we can get average latency\n",
        "total_samples = 1000\n",
        "\n",
        "# ONNX opset version\n",
        "opset_version=11"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1648061758193
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify some model configuration variables."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tuned model from https://huggingface.co/models?search=squad\n",
        "model_name_or_path = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "max_seq_length = 128\n",
        "doc_stride = 128\n",
        "max_query_length = 64"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1648061758697
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start to load model from pretrained. This step could take a few minutes. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# The following code is adapted from HuggingFace transformers\n",
        "# https://github.com/huggingface/transformers/blob/master/examples/run_squad.py\n",
        "\n",
        "from transformers import (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "config_class, model_class, tokenizer_class = (BertConfig, BertForQuestionAnswering, BertTokenizer)\n",
        "config = config_class.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
        "model = model_class.from_pretrained(model_name_or_path,\n",
        "                                    from_tf=False,\n",
        "                                    config=config,\n",
        "                                    cache_dir=cache_dir)\n",
        "# load some examples\n",
        "from transformers.data.processors.squad import SquadV1Processor\n",
        "\n",
        "processor = SquadV1Processor()\n",
        "examples = processor.get_dev_examples(None, filename=predict_file)\n",
        "\n",
        "from transformers import squad_convert_examples_to_features\n",
        "features, dataset = squad_convert_examples_to_features( \n",
        "            examples=examples[:total_samples], # convert enough examples for this notebook\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=False,\n",
        "            return_dataset='pt'\n",
        "        )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 48/48 [00:03<00:00, 14.88it/s]\nconvert squad examples to features: 100%|██████████| 1000/1000 [00:07<00:00, 125.25it/s]\nadd example index and unique id: 100%|██████████| 1000/1000 [00:00<00:00, 479458.62it/s]\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1648061812501
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Export the loaded model ##\n",
        "Once the model is loaded, we can export the loaded PyTorch model to ONNX."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./onnx\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)   \n",
        "export_model_path = os.path.join(output_dir, 'bert-base-cased-squad_opset{}.onnx'.format(opset_version))\n",
        "\n",
        "import torch\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "\n",
        "# Get the first example data to run the model and export it to ONNX\n",
        "data = dataset[0]\n",
        "inputs = {\n",
        "    'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
        "    'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
        "    'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
        "}\n",
        "\n",
        "# Set model to inference mode, which is required before exporting the model because some operators behave differently in \n",
        "# inference and training mode.\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "if enable_overwrite or not os.path.exists(export_model_path):\n",
        "    with torch.no_grad():\n",
        "        symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
        "        torch.onnx.export(model,                                            # model being run\n",
        "                          args=tuple(inputs.values()),                      # model input (or a tuple for multiple inputs)\n",
        "                          f=export_model_path,                              # where to save the model (can be a file or file-like object)\n",
        "                          opset_version=opset_version,                      # the ONNX version to export the model to\n",
        "                          do_constant_folding=True,                         # whether to execute constant folding for optimization\n",
        "                          input_names=['input_ids',                         # the model's input names\n",
        "                                       'input_mask', \n",
        "                                       'segment_ids'],\n",
        "                          output_names=['start', 'end'],                    # the model's output names\n",
        "                          dynamic_axes={'input_ids': symbolic_names,        # variable length axes\n",
        "                                        'input_mask' : symbolic_names,\n",
        "                                        'segment_ids' : symbolic_names,\n",
        "                                        'start' : symbolic_names,\n",
        "                                        'end' : symbolic_names})\n",
        "        print(\"Model exported at \", export_model_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model exported at  ./onnx/bert-base-cased-squad_opset11.onnx\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1648061841524
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. PyTorch Inference ##\n",
        "Use PyTorch to evaluate an example input for comparison purpose."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Measure the latency. It is not accurate using Jupyter Notebook, it is recommended to use standalone python script.\n",
        "latency = []\n",
        "with torch.no_grad():\n",
        "    for i in range(total_samples):\n",
        "        data = dataset[i]\n",
        "        inputs = {\n",
        "            'input_ids':      data[0].to(device).reshape(1, max_seq_length),\n",
        "            'attention_mask': data[1].to(device).reshape(1, max_seq_length),\n",
        "            'token_type_ids': data[2].to(device).reshape(1, max_seq_length)\n",
        "        }\n",
        "        start = time.time()\n",
        "        outputs = model(**inputs)\n",
        "        latency.append(time.time() - start)\n",
        "print(\"PyTorch {} Inference time = {} ms\".format(device.type, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "PyTorch cuda Inference time = 24.43 ms\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1648061866470
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Inference ONNX Model with ONNX Runtime ##\n",
        "\n",
        "### CUDA and cuDNN Path\n",
        "onnxruntime-gpu has dependency on [CUDA](https://developer.nvidia.com/cuda-downloads) and [cuDNN](https://developer.nvidia.com/cudnn). Required CUDA version can be found [here](http://www.onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to True when onnxruntime (like onnxruntime-gpu 1.0.0 ~ 1.1.2) cannot be imported.\n",
        "add_cuda_path = False\n",
        "\n",
        "# For Linux, see https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#environment-setup\n",
        "# Below is example for Windows\n",
        "if add_cuda_path:\n",
        "    cuda_dir = 'D:/NVidia/CUDA/v11.0/bin'\n",
        "    cudnn_dir = 'D:/NVidia/CUDA/v11.0/bin'\n",
        "    if not (os.path.exists(cuda_dir) and os.path.exists(cudnn_dir)):\n",
        "        raise ValueError(\"Please specify correct path for CUDA and cuDNN. Otherwise onnxruntime cannot be imported.\")\n",
        "    else:\n",
        "        if cuda_dir == cudnn_dir:\n",
        "            os.environ[\"PATH\"] = cuda_dir + ';' + os.environ[\"PATH\"]\n",
        "        else:\n",
        "            os.environ[\"PATH\"] = cuda_dir + ';' + cudnn_dir + ';' + os.environ[\"PATH\"]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1648061915197
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to inference the model with ONNX Runtime."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\r\n",
        "import onnxruntime\r\n",
        "import numpy\r\n",
        "\r\n",
        "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\r\n",
        "device_name = 'gpu'\r\n",
        "\r\n",
        "sess_options = onnxruntime.SessionOptions()\r\n",
        "\r\n",
        "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\r\n",
        "# Note that this will increase session creation time so enable it for debugging only.\r\n",
        "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\r\n",
        "\r\n",
        "# Please change the value according to best setting in Performance Test Tool result.\r\n",
        "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\r\n",
        "\r\n",
        "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\r\n",
        "\r\n",
        "latency = []\r\n",
        "for i in range(total_samples):\r\n",
        "    data = dataset[i]\r\n",
        "    # TODO: use IO Binding (see https://www.onnxruntime.ai/python/api_summary.html) to improve performance.\r\n",
        "    ort_inputs = {\r\n",
        "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\r\n",
        "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\r\n",
        "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\r\n",
        "    }\r\n",
        "    start = time.time()\r\n",
        "    ort_outputs = session.run(None, ort_inputs)\r\n",
        "    latency.append(time.time() - start)\r\n",
        "    \r\n",
        "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2022-03-23 18:58:57.992986004 [W:onnxruntime:, inference_session.cc:1546 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "OnnxRuntime gpu Inference time = 11.58 ms\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1648061958521
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with ONNX Runtime IO Binding"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import onnxruntime\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "assert 'CUDAExecutionProvider' in onnxruntime.get_available_providers()\n",
        "device_name = 'gpu'\n",
        "\n",
        "sess_options = onnxruntime.SessionOptions()\n",
        "\n",
        "# Optional: store the optimized graph and view it using Netron to verify that model is fully optimized.\n",
        "# Note that this will increase session creation time so enable it for debugging only.\n",
        "sess_options.optimized_model_filepath = os.path.join(output_dir, \"optimized_model_{}.onnx\".format(device_name))\n",
        "\n",
        "# Please change the value according to best setting in Performance Test Tool result.\n",
        "sess_options.intra_op_num_threads=psutil.cpu_count(logical=True)\n",
        "\n",
        "# Bind inputs and outputs to their PyTorch native tensors\n",
        "session = onnxruntime.InferenceSession(export_model_path, sess_options, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "binding = session.io_binding()\n",
        "\n",
        "latency = []\n",
        "for i in range(total_samples):\n",
        "    data = dataset[i]\n",
        "\n",
        "    input_ids = data[0].type(dtype=torch.int64).to(device).reshape(1, max_seq_length).contiguous()\n",
        "    input_mask = data[1].type(dtype=torch.int64).to(device).reshape(1, max_seq_length).contiguous()\n",
        "    segment_ids = data[2].type(dtype=torch.int64).to(device).reshape(1, max_seq_length).contiguous()\n",
        "\n",
        "    INPUT_TYPE=numpy.int64\n",
        "\n",
        "    binding.bind_input(\n",
        "        name='input_ids',\n",
        "        device_type='cuda',\n",
        "        device_id=0,\n",
        "        element_type=INPUT_TYPE,\n",
        "        shape=tuple(input_ids.shape),\n",
        "        buffer_ptr=input_ids.data_ptr(),\n",
        "        )\n",
        "\n",
        "    binding.bind_input(\n",
        "        name='input_mask',\n",
        "        device_type='cuda',\n",
        "        device_id=0,\n",
        "        element_type=INPUT_TYPE,\n",
        "        shape=tuple(input_mask.shape),\n",
        "        buffer_ptr=input_mask.data_ptr(),\n",
        "        )\n",
        "\n",
        "    binding.bind_input(\n",
        "        name='segment_ids',\n",
        "        device_type='cuda',\n",
        "        device_id=0,\n",
        "        element_type=INPUT_TYPE,\n",
        "        shape=tuple(segment_ids.shape),\n",
        "        buffer_ptr=segment_ids.data_ptr(),\n",
        "        )\n",
        "\n",
        "    ## Allocate the PyTorch tensors for the model output\n",
        "    output_shape = (1,max_seq_length)\n",
        "    start_tensor = torch.empty(output_shape, dtype=torch.float32, device='cuda:0').contiguous()\n",
        "    end_tensor = torch.empty(output_shape, dtype=torch.float32, device='cuda:0').contiguous()\n",
        "    binding.bind_output(\n",
        "        name='start',\n",
        "        device_type='cuda',\n",
        "        device_id=0,\n",
        "        element_type=numpy.float32,\n",
        "        shape=tuple(start_tensor.shape),\n",
        "        buffer_ptr=start_tensor.data_ptr()\n",
        "    )\n",
        "\n",
        "    binding.bind_output(\n",
        "        name='end',\n",
        "        device_type='cuda',\n",
        "        device_id=0,\n",
        "        element_type=numpy.float32,\n",
        "        shape=tuple(end_tensor.shape),\n",
        "        buffer_ptr=end_tensor.data_ptr()\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    session.run_with_iobinding(binding)\n",
        "\n",
        "    latency.append(time.time() - start)\n",
        "    \n",
        "print(\"OnnxRuntime {} Inference time = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "OnnxRuntime gpu Inference time = 11.47 ms\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1648062146496
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the output of PyTorch and ONNX Runtime. We can see some results are not close. It is because ONNX Runtime uses some approximation in CUDA optimization. Based on our evaluation on SQuAD data set, F1 score is on par for models before and after optimization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Verifying correctness *****\")\n",
        "for i in range(2):    \n",
        "    print('PyTorch and ONNX Runtime output {} are close:'.format(i), numpy.allclose(ort_outputs[i], outputs[i].cpu(), rtol=1e-02, atol=1e-02))\n",
        "    diff = ort_outputs[i] - outputs[i].cpu().numpy()\n",
        "    max_diff = numpy.max(numpy.abs(diff))\n",
        "    avg_diff = numpy.average(numpy.abs(diff))\n",
        "    print(f'maximum_diff={max_diff} average_diff={avg_diff}')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1646962409843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference with Actual Sequence Length\n",
        "Note that ONNX model is exported using dynamic length axis. It is recommended to use actual sequence input without padding instead of fixed length input for best performance. Let's see how it can be applied to this model.\n",
        "\n",
        "From an example input below, we can see zero padding at the end of each sequence."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# An example input (we can see padding). From attention_mask, we can deduce the actual length.\n",
        "inputs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1646962410195
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original sequence length is 128. After removing paddings, the sequence length is reduced. Input with smaller sequence length need less computation, thus we can see there is improvement on inference latency. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "\n",
        "latency = []\n",
        "lengths = []\n",
        "for i in range(total_samples):\n",
        "    data = dataset[i]\n",
        "    # Instead of using fixed length (128), we can use actual sequence length (less than 128), which helps to get better performance.\n",
        "    actual_sequence_length = sum(data[1].numpy())\n",
        "    lengths.append(actual_sequence_length)\n",
        "    opt_inputs = {\n",
        "        'input_ids':  data[0].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
        "        'input_mask': data[1].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length),\n",
        "        'segment_ids': data[2].numpy()[:actual_sequence_length].reshape(1, actual_sequence_length)\n",
        "    }\n",
        "    start = time.time()\n",
        "    opt_outputs = session.run(None, opt_inputs)\n",
        "    latency.append(time.time() - start)\n",
        "print(\"Average length\", statistics.mean(lengths))\n",
        "print(\"OnnxRuntime {} Inference time with actual sequence length = {} ms\".format(device_name, format(sum(latency) * 1000 / len(latency), '.2f')))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646962418498
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the output and see whether the results are close.\n",
        "\n",
        "**Note**: Need end-to-end evaluation on performance and accuracy if you use this strategy."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** Comparing results with/without paddings *****\")\n",
        "for i in range(2):\n",
        "    print('Output {} are close:'.format(i), numpy.allclose(opt_outputs[i], ort_outputs[i][:,:len(opt_outputs[i][0])], rtol=1e-03, atol=1e-03))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646962418730
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Offline Optimization and Test Tools\n",
        "\n",
        "It is recommended to try [OnnxRuntime Transformer Model Optimization Tool](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers) on the exported ONNX models. It could help verify whether the model can be fully optimized, and get performance test results."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformer Optimizer\n",
        "\n",
        "Although OnnxRuntime could optimize Bert model exported by PyTorch. Sometime, model cannot be fully optimized due to different reasons:\n",
        "* A new subgraph pattern is generated by new version of export tool, and the pattern is not covered by older version of OnnxRuntime. \n",
        "* The exported model uses dynamic axis and this makes it harder for shape inference of the graph. That blocks some optimization to be applied.\n",
        "* Some optimization is better to be done offline. Like change input tensor type from int64 to int32 to avoid extra Cast nodes, or convert model to float16 to achieve better performance in V100 or T4 GPU.\n",
        "\n",
        "We have python script **optimizer.py**, which is more flexible in graph pattern matching and model conversion (like float32 to float16). You can also use it to verify whether a Bert model is fully optimized.\n",
        "\n",
        "In this example, we can see that it introduces optimization that is not provided by onnxruntime: SkipLayerNormalization and bias fusion, which is not fused in OnnxRuntime due to shape inference as mentioned.\n",
        "\n",
        "It will also tell whether the model is fully optimized or not. If not, that means you might need change the script to fuse some new pattern of subgraph.\n",
        "\n",
        "Example Usage:\n",
        "```\n",
        "from onnxruntime.transformers import optimizer\n",
        "optimized_model = optimizer.optimize_model(export_model_path, model_type='bert', num_heads=12, hidden_size=768)\n",
        "optimized_model.save_model_to_file(optimized_model_path)\n",
        "```\n",
        "\n",
        "You can also use command line like the following:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Float32 Model\n",
        "Let us optimize the ONNX model using the script. The first example will output model with float32 to store weights. This is the choice for most GPUs without Tensor Core.\n",
        "\n",
        "If your GPU (like V100 or T4) has Tensor Core, jump to [Float16 Model](#6.-Model-Optimization-with-Float16) section since that will give you better performance than Float32 model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_fp32_model_path = './onnx/bert-base-cased-squad_opt_{}_fp32.onnx'.format('gpu' if use_gpu else 'cpu')\n",
        "\n",
        "!{sys.executable} -m onnxruntime.transformers.optimizer --input $export_model_path --output $optimized_fp32_model_path"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646962514319
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimized Graph\n",
        "We can open the optimized model using [Netron](https://github.com/lutzroeder/netron) to visualize.\n",
        "\n",
        "The graph is like the following:\n",
        "<img src='images/optimized_bert_gpu.png'>\n",
        "\n",
        "Sometime, optimized graph is slightly different. For example, FastGelu is replaced by BiasGelu for CPU inference; When the option --input_int32 is used, Cast nodes for inputs are removed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import netron\n",
        "\n",
        "# change it to True if want to view the optimized model in browser\n",
        "enable_netron = True\n",
        "if enable_netron:\n",
        "    # If you encounter error \"access a socket in a way forbidden by its access permissions\", install Netron as standalone application instead.\n",
        "    netron.start(optimized_fp32_model_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646962514667
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Test Tool\n",
        "\n",
        "The following will create 1000 random inputs of batch_size 1 and sequence length 128, then measure the average latency and throughput numbers.\n",
        "\n",
        "Note that the test uses fixed sequence length. If you use [dynamic sequence length](#Inference-with-Actual-Sequence-Length), actual performance depends on the distribution of sequence length.\n",
        "\n",
        "**Attention**: Latency numbers from Jupyter Notebook are not accurate. See [Attional Info](#7.-Additional-Info) for more info."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
        "\n",
        "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 $GPU_OPTION"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646962750555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the summary file and take a look. Note that blank value in OMP_NUM_THREADS or OMP_WAIT_POLICY means the environment variable does not exist."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob     \n",
        "import pandas\n",
        "latest_result_file = max(glob.glob(\"./onnx/perf_results_GPU_B1_S128_*.txt\"), key=os.path.getmtime)\n",
        "result_data = pandas.read_table(latest_result_file)\n",
        "print(\"Float32 model perf results from\", latest_result_file)\n",
        "# Remove some columns that have same values for all rows.\n",
        "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
        "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
        "result_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646962751009
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above result, we can see that latency is very close for different settings. The default setting (intra_op_num_threads=0, OMP_NUM_THREADS and OMP_WAIT_POLICY does not exist) performs the best. \n",
        "\n",
        "### Model Results Comparison Tool\n",
        "\n",
        "When a BERT model is optimized, some approximation is used in calculation. If your BERT model has three inputs, a script compare_bert_results.py can be used to do a quick verification. The tool will generate some fake input data, and compare the inference outputs of the original and optimized models. If outputs are all close, it is safe to use the optimized model.\n",
        "\n",
        "For GPU inference, the absolute or relative difference is larger than those numbers of CPU inference. Note that slight difference in output will not impact final result. We did end-to-end evaluation using SQuAD data set using a fine-tuned squad model, and F1 score is almost the same before/after optimization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m onnxruntime.transformers.compare_bert_results --baseline_model $export_model_path --optimized_model $optimized_fp32_model_path --batch_size 1 --sequence_length 128 --samples 100 --rtol 0.01 --atol 0.01 $GPU_OPTION"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Optimization with Float16\n",
        "\n",
        "The optimizer.py script have an option **--float16** to convert model to use float16 to store weights. After the conversion, it could be faster to run in GPU with tensor cores like V100 or T4.\n",
        "\n",
        "Let's run tools to measure the performance on V100. The results show significant performance improvement: latency is about 3.4 ms for float32 model, and 1.8 ms for float16 model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_fp16_model_path = './onnx/bert-base-cased-squad_opt_{}_fp16.onnx'.format('gpu' if use_gpu else 'cpu')\n",
        "!{sys.executable} -m onnxruntime.transformers.optimizer --input $export_model_path --output $optimized_fp16_model_path --float16"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646963032078
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
        "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_fp16_model_path --batch_size 1 --sequence_length 128 --samples 1000 --test_times 1 $GPU_OPTION"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646963764635
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob     \n",
        "import pandas\n",
        "latest_result_file = max(glob.glob(\"./onnx/perf_results_GPU_B1_S128_*.txt\"), key=os.path.getmtime)\n",
        "result_data = pandas.read_table(latest_result_file)\n",
        "print(\"Float32 model perf results from\", latest_result_file)\n",
        "# Remove some columns that have same values for all rows.\n",
        "columns_to_remove = ['model', 'graph_optimization_level', 'batch_size', 'sequence_length', 'test_cases', 'test_times', 'use_gpu']\n",
        "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
        "result_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646963880714
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Throughput Tuning\n",
        "\n",
        "Some application need best throughput under some constraint on latency. This can be done by testing performance of different batch sizes. The tool could help on this.\n",
        "\n",
        "Here is an example that check the performance of multiple batch sizes (1, 2, 4, 8, 16, 32 and 64) using default settings."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "GPU_OPTION = '--use_gpu' if use_gpu else ''\n",
        "THREAD_SETTING = '--intra_op_num_threads 3'\n",
        "!{sys.executable} -m onnxruntime.transformers.bert_perf_test --model $optimized_fp16_model_path --batch_size 1 2 4 8 16 32 --sequence_length 128 --samples 1000 --test_times 1 $THREAD_SETTING $GPU_OPTION"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646964175818
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob     \n",
        "import pandas\n",
        "latest_result_file = max(glob.glob(\"./onnx/perf_results_*.txt\"), key=os.path.getmtime)\n",
        "result_data = pandas.read_table(latest_result_file)\n",
        "print(\"Float16 model summary from\", latest_result_file)\n",
        "columns_to_remove = ['model', 'graph_optimization_level', 'test_cases', 'test_times', 'use_gpu', 'sequence_length']\n",
        "columns_to_remove.extend(['intra_op_num_threads'])\n",
        "result_data.drop(columns_to_remove, axis=1, inplace=True)\n",
        "result_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "gather": {
          "logged": 1646964258884
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Additional Info\n",
        "\n",
        "Note that running Jupyter Notebook has significant impact on performance result. You can close Jupyter Notebook and other applications, then run the performance test in a console to get more accurate performance numbers.\n",
        "\n",
        "We have a [benchmark script](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/run_benchmark.sh). It is recommended to use it measure inference speed of OnnxRuntime.\n",
        "\n",
        "[OnnxRuntime C API](https://github.com/microsoft/onnxruntime/blob/master/docs/C_API.md) could get slightly better performance than python API. If you use C API in inference, you can use OnnxRuntime_Perf_Test.exe built from source to measure performance instead.\n",
        "\n",
        "Here is the machine configuration that generated the above results. You might get slower or faster result according to your hardware."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m onnxruntime.transformers.machine_info --silent"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "gpu-kernel",
      "language": "python",
      "display_name": "gpu-kernel"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "gpu-kernel"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}