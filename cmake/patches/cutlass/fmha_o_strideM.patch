diff --git a/examples/41_fused_multi_head_attention/kernel_forward.h b/examples/41_fused_multi_head_attention/kernel_forward.h
index 3f85953b..8ef1c38b 100644
--- a/examples/41_fused_multi_head_attention/kernel_forward.h
+++ b/examples/41_fused_multi_head_attention/kernel_forward.h
@@ -162,8 +162,9 @@ struct AttentionKernel {
     int32_t num_batches;
     int32_t num_heads;
 
+    // https://github.com/NVIDIA/cutlass/issues/771
     CUTLASS_HOST_DEVICE int32_t o_strideM() const {
-      return head_dim_value;
+      return head_dim_value * num_heads;
     }
 
     // Moves pointers to what we should process
@@ -930,7 +931,7 @@ __global__ void __launch_bounds__(AK::kNumThreads, AK::kMinBlocksPerSm)
 // Enable the right one based on __CUDA_ARCH__
 #ifndef __CUDA_ARCH__
 #elif __CUDA_ARCH__ < 500
-#error "Need cuda arch at least 5.0"
+//#error "Need cuda arch at least 5.0"
 #elif __CUDA_ARCH__ < 700
 #undef INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50
 #define INSTANTIATE_ATTENTION_KERNEL_FORWARD_SM50(...) \
