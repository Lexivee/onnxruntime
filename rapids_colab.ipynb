{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rapids-colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerberusdevops/onnxruntime/blob/master/rapids_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD",
        "colab_type": "text"
      },
      "source": [
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik8Wv2K0uDfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pynvml\n",
        "\n",
        "pynvml.nvmlInit()\n",
        "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "device_name = pynvml.nvmlDeviceGetName(handle)\n",
        "\n",
        "if device_name != b'Tesla T4':\n",
        "  raise Exception(\"\"\"\n",
        "    Unfortunately this instance does not have a T4 GPU.\n",
        "    \n",
        "    Please make sure you've configured Colab to request a GPU instance type.\n",
        "    \n",
        "    Sometimes Colab allocates a Tesla K80 instead of a T4. Resetting the instance.\n",
        "\n",
        "    If you get a K80 GPU, try Runtime -> Reset all runtimes...\n",
        "  \"\"\")\n",
        "else:\n",
        "  print('Woo! You got the right kind of GPU!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtNdk7PSafKP",
        "colab_type": "text"
      },
      "source": [
        "#Setup:\n",
        "\n",
        "1. Install most recent Miniconda release compatible with Google Colab's Python install  (3.6.7)\n",
        "2. Install RAPIDS libraries\n",
        "3. Set necessary environment variables\n",
        "4. Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0jdXBRiDSzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# intall miniconda\n",
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# install RAPIDS packages\n",
        "!conda install -q -y --prefix /usr/local -c conda-forge \\\n",
        "  -c rapidsai-nightly/label/cuda10.0 -c nvidia/label/cuda10.0 \\\n",
        "  cudf cuml\n",
        "\n",
        "# set environment vars\n",
        "import sys, os, shutil\n",
        "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "\n",
        "# copy .so files to current working dir\n",
        "for fn in ['libcudf.so', 'librmm.so']:\n",
        "  shutil.copy('/usr/local/lib/'+fn, os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oOCJ4NYMjY7",
        "colab_type": "text"
      },
      "source": [
        "# cuDF and cuML Examples #\n",
        "\n",
        "Now you can run code! \n",
        "\n",
        "What follows are basic examples where all processing takes place on the GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V38dg-oUJtEO",
        "colab_type": "text"
      },
      "source": [
        "#[cuDF](https://github.com/rapidsai/cudf)#\n",
        "\n",
        "Load a dataset into a GPU memory resident DataFrame and perform a basic calculation.\n",
        "\n",
        "Everything from CSV parsing to calculating tip percentage and computing a grouped average is done on the GPU.\n",
        "\n",
        "_Note_: You must import nvstrings and nvcategory before cudf, else you'll get errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwaJSKuswsNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nvstrings, nvcategory, cudf\n",
        "import io, requests\n",
        "\n",
        "# download CSV file from GitHub\n",
        "url=\"https://github.com/plotly/datasets/raw/master/tips.csv\"\n",
        "content = requests.get(url).content.decode('utf-8')\n",
        "\n",
        "# read CSV from memory\n",
        "tips_df = cudf.read_csv(io.StringIO(content))\n",
        "tips_df['tip_percentage'] = tips_df['tip']/tips_df['total_bill']*100\n",
        "\n",
        "# display average tip by dining party size\n",
        "print(tips_df.groupby('size').tip_percentage.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3UZJdUJqlT",
        "colab_type": "text"
      },
      "source": [
        "#[cuML](https://github.com/rapidsai/cuml)#\n",
        "\n",
        "This snippet loads a \n",
        "\n",
        "As above, all calculations are performed on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCE8WhO3HpL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cuml\n",
        "\n",
        "# Create and populate a GPU DataFrame\n",
        "df_float = cudf.DataFrame()\n",
        "df_float['0'] = [1.0, 2.0, 5.0]\n",
        "df_float['1'] = [4.0, 2.0, 1.0]\n",
        "df_float['2'] = [4.0, 2.0, 1.0]\n",
        "\n",
        "# Setup and fit clusters\n",
        "dbscan_float = cuml.DBSCAN(eps=1.0, min_samples=1)\n",
        "dbscan_float.fit(df_float)\n",
        "\n",
        "print(dbscan_float.labels_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K",
        "colab_type": "text"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-extended"
      ]
    }
  ]
}